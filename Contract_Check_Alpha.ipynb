{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import fitz\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "import logging\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    load_dotenv()\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API key not found in environment variables.\") \n",
    "\n",
    "    logging.info(\"Environment variables loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading environment variables: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Create Contract Checking Assistant\n",
    "instructions = \"\"\"\n",
    "You are a Contract Checking Assistant. Your task is to receive written contracts, check them against specified regulations, identify missing parts, suggest modifications, and provide recommendations for improvement.\n",
    "\n",
    "When a contract is provided, follow these steps:\n",
    "\n",
    "1. **Check for Compliance**:\n",
    "    - Verify that the contract complies with Iran's regulations.\n",
    "    - Check compliance with ICC and Incoterms regulations.\n",
    "2. **Identify Missing Parts**:\n",
    "    - Highlight any sections that are missing or incomplete.\n",
    "3. **Suggest Modifications**:\n",
    "    - Recommend changes to ensure the contract meets all legal and regulatory requirements.\n",
    "4. **Provide Improvements**:\n",
    "    - Offer suggestions on how to improve the contract for clarity, fairness, and comprehensiveness.\n",
    "5. **Compare with Similar Contracts**:\n",
    "    - Compare the contract with similar contracts to identify common practices and potential improvements.\n",
    "6. **Identify Weaknesses and Challenges**:\n",
    "    - Find weaknesses and challenging points in the contract and suggest modifications for these parts.\n",
    "\n",
    "Your responses should be clear, concise, and professional. Always provide detailed explanations for your suggestions and ensure that your feedback is actionable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_tvQRDI4xxvg1r73jHrNwJu6y', created_at=1727877357, description=None, instructions=\"\\nYou are a Contract Checking Assistant. Your task is to receive written contracts, check them against specified regulations, identify missing parts, suggest modifications, and provide recommendations for improvement.\\n\\nWhen a contract is provided, follow these steps:\\n\\n1. **Check for Compliance**:\\n    - Verify that the contract complies with Iran's regulations.\\n    - Check compliance with ICC and Incoterms regulations.\\n2. **Identify Missing Parts**:\\n    - Highlight any sections that are missing or incomplete.\\n3. **Suggest Modifications**:\\n    - Recommend changes to ensure the contract meets all legal and regulatory requirements.\\n4. **Provide Improvements**:\\n    - Offer suggestions on how to improve the contract for clarity, fairness, and comprehensiveness.\\n5. **Compare with Similar Contracts**:\\n    - Compare the contract with similar contracts to identify common practices and potential improvements.\\n6. **Identify Weaknesses and Challenges**:\\n    - Find weaknesses and challenging points in the contract and suggest modifications for these parts.\\n\\nYour responses should be clear, concise, and professional. Always provide detailed explanations for your suggestions and ensure that your feedback is actionable.\\n\", metadata={'project': 'Contract Review'}, model='gpt-4o-mini', name='Contract_Check_Assistant_V4', object='assistant', tools=[FileSearchTool(type='file_search', file_search=FileSearch(max_num_results=None, ranking_options=FileSearchRankingOptions(score_threshold=0.0, ranker='default_2024_08_21'))), CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=[]), file_search=ToolResourcesFileSearch(vector_store_ids=[])), top_p=1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Contract_Checking_Assistant\n",
    "Contract_Checking_Assistant = client.beta.assistants.create(\n",
    "    name=\"Contract_Check_Assistant_V4\",\n",
    "    instructions=instructions,\n",
    "    model=model,\n",
    "    tools=[\n",
    "        {\"type\": \"file_search\"},\n",
    "        {\"type\": \"code_interpreter\"},\n",
    "    ],\n",
    "    metadata={\"project\": \"Contract Review\"}\n",
    ")\n",
    "\n",
    "Contract_Checking_Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStore(id='vs_iPxWuQl6r96nIPu07m5flEeK', created_at=1727877515, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1727877515, metadata={}, name='The Rules', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store called \"The Rules\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"The Rules\")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﺑﺎب اول\\nﺗﺠﺎر و ﻣﻌﺎﻣﻼت ﺗﺠﺎرﺗﯽ\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document\n",
    "import logging\n",
    "\n",
    "def extract_text_from_word(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from Word file: {e}\")\n",
    "        raise\n",
    "    return text\n",
    "\n",
    "# Upload regulations to the vector store\n",
    "regulations_file_path = \"./iran_trade_rules.docx\"\n",
    "try:\n",
    "    regulations_text = extract_text_from_word(regulations_file_path)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error extracting text from regulations file: {e}\")\n",
    "    raise\n",
    "regulations_text[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Text Embedding with TensorFlow\n",
    "def embed_text_large(text):\n",
    "    try:\n",
    "        # Load the tokenizer and model for XLM-RoBERTa Large\n",
    "        model_name = \"xlm-roberta-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = TFAutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "        \n",
    "        # Get the embeddings from the model (CLS token represents the whole sentence)\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        return embedding.numpy().tolist()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error embedding text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Embed the text using TensorFlow\n",
    "try:\n",
    "    embedding_large = embed_text_large(regulations_text)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error embedding regulations text: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedded text to a file\n",
    "def save_embeddings_to_file(embeddings, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(embeddings, f)\n",
    "        logging.info(f\"Embeddings saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving embeddings to file: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file to the vector store\n",
    "def upload_to_vector_store(file_path, vector_store_id):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            embeddings = json.load(f)\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        vector_store = client.vector_stores.get(vector_store_id)\n",
    "        vector_store.upload(embeddings)\n",
    "        logging.info(f\"Embeddings uploaded to vector store {vector_store_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading embeddings to vector store: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./embedded_iran_trade_rules.json\"\n",
    "save_embeddings_to_file(embedding_large,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "file_stream = open(file_path, \"rb\")\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=[file_stream]\n",
    ")\n",
    "file_stream.close()\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Update the assistant with the vector store\n",
    "Contract_Checking_Assistant = client.beta.assistants.update(\n",
    "    assistant_id=Contract_Checking_Assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thread_qWbTKZYSuwToACLpXW2hVmp4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample thread with a non-empty message content\n",
    "thread = client.beta.threads.create()\n",
    "thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contract_Checking_Assistant = client.beta.assistants.update(\n",
    "    assistant_id=Contract_Checking_Assistant.id,\n",
    "    tools=[\n",
    "        {\"type\": \"file_search\"},\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"contract_check\",\n",
    "                \"description\": \"Check the contract against specified regulations.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"contract_text\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The text of the contract to be checked.\"\n",
    "                        },\n",
    "                        \"regulations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"description\": \"A list of regulations to check against.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"contract_text\", \"regulations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
