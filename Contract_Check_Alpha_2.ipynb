{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "import logging\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import docx \n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State\n",
    "import dash_uploader as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    load_dotenv()\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API key not found in environment variables.\") \n",
    "\n",
    "    logging.info(\"Environment variables loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading environment variables: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Create Contract Checking Assistant\n",
    "instructions = \"\"\"\n",
    "You are a Contract Checking Assistant. Your task is to receive written contracts, check them against specified regulations, identify missing parts, suggest modifications, and provide recommendations for improvement.\n",
    "\n",
    "When a contract is provided, follow these steps:\n",
    "\n",
    "1. **Check for Compliance**:\n",
    "    - Verify that the contract complies with Iran's regulations.\n",
    "    - Check compliance with ICC and Incoterms regulations.\n",
    "2. **Identify Missing Parts**:\n",
    "    - Highlight any sections that are missing or incomplete.\n",
    "3. **Suggest Modifications**:\n",
    "    - Recommend changes to ensure the contract meets all legal and regulatory requirements.\n",
    "4. **Provide Improvements**:\n",
    "    - Offer suggestions on how to improve the contract for clarity, fairness, and comprehensiveness.\n",
    "5. **Compare with Similar Contracts**:\n",
    "    - Compare the contract with similar contracts to identify common practices and potential improvements.\n",
    "6. **Identify Weaknesses and Challenges**:\n",
    "    - Find weaknesses and challenging points in the contract and suggest modifications for these parts.\n",
    "\n",
    "Your responses should be clear, concise, and professional. Always provide detailed explanations for your suggestions and ensure that your feedback is actionable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_3R70lBiU5AyYDjie9pTRGeZj', created_at=1727959588, description=None, instructions=\"\\nYou are a Contract Checking Assistant. Your task is to receive written contracts, check them against specified regulations, identify missing parts, suggest modifications, and provide recommendations for improvement.\\n\\nWhen a contract is provided, follow these steps:\\n\\n1. **Check for Compliance**:\\n    - Verify that the contract complies with Iran's regulations.\\n    - Check compliance with ICC and Incoterms regulations.\\n2. **Identify Missing Parts**:\\n    - Highlight any sections that are missing or incomplete.\\n3. **Suggest Modifications**:\\n    - Recommend changes to ensure the contract meets all legal and regulatory requirements.\\n4. **Provide Improvements**:\\n    - Offer suggestions on how to improve the contract for clarity, fairness, and comprehensiveness.\\n5. **Compare with Similar Contracts**:\\n    - Compare the contract with similar contracts to identify common practices and potential improvements.\\n6. **Identify Weaknesses and Challenges**:\\n    - Find weaknesses and challenging points in the contract and suggest modifications for these parts.\\n\\nYour responses should be clear, concise, and professional. Always provide detailed explanations for your suggestions and ensure that your feedback is actionable.\\n\", metadata={'project': 'Contract Review'}, model='gpt-4o-mini', name='Contract_Check_Assistant_alpha', object='assistant', tools=[FileSearchTool(type='file_search', file_search=FileSearch(max_num_results=None, ranking_options=FileSearchRankingOptions(score_threshold=0.0, ranker='default_2024_08_21'))), CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=[]), file_search=ToolResourcesFileSearch(vector_store_ids=[])), top_p=1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Contract_Checking_Assistant\n",
    "Contract_Checking_Assistant = client.beta.assistants.create(\n",
    "    name=\"Contract_Check_Assistant_alpha\",\n",
    "    instructions=instructions,\n",
    "    model=model,\n",
    "    tools=[\n",
    "        {\"type\": \"file_search\"},\n",
    "        {\"type\": \"code_interpreter\"},\n",
    "    ],\n",
    "    metadata={\"project\": \"Contract Review\"}\n",
    ")\n",
    "\n",
    "Contract_Checking_Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStore(id='vs_lv2zMVM2ggcYodr5MZA4oI6m', created_at=1727959649, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1727959649, metadata={}, name='The Rules', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store called \"The Rules\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"The Rules\")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﺑﺎب اول\\nﺗﺠﺎر و ﻣﻌﺎﻣﻼت ﺗﺠﺎرﺗﯽ\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document\n",
    "import logging\n",
    "\n",
    "def extract_text_from_word(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from Word file: {e}\")\n",
    "        raise\n",
    "    return text\n",
    "\n",
    "# Upload regulations to the vector store\n",
    "regulations_file_path = \"./iran_trade_rules.docx\"\n",
    "try:\n",
    "    regulations_text = extract_text_from_word(regulations_file_path)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error extracting text from regulations file: {e}\")\n",
    "    raise\n",
    "regulations_text[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Text Embedding with TensorFlow\n",
    "def embed_text_large(text):\n",
    "    try:\n",
    "        # Load the tokenizer and model for XLM-RoBERTa Large\n",
    "        model_name = \"xlm-roberta-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = TFAutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "        \n",
    "        # Get the embeddings from the model (CLS token represents the whole sentence)\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        return embedding.numpy().tolist()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error embedding text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Embed the text using TensorFlow\n",
    "try:\n",
    "    embedding_large = embed_text_large(regulations_text)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error embedding regulations text: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedded text to a file\n",
    "def save_embeddings_to_file(embeddings, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(embeddings, f)\n",
    "        logging.info(f\"Embeddings saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving embeddings to file: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file to the vector store\n",
    "def upload_to_vector_store(file_path, vector_store_id):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            embeddings = json.load(f)\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        vector_store = client.vector_stores.get(vector_store_id)\n",
    "        vector_store.upload(embeddings)\n",
    "        logging.info(f\"Embeddings uploaded to vector store {vector_store_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading embeddings to vector store: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./embedded_iran_trade_rules.json\"\n",
    "save_embeddings_to_file(embedding_large,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "file_stream = open(file_path, \"rb\")\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=[file_stream]\n",
    ")\n",
    "file_stream.close()\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Update the assistant with the vector store\n",
    "Contract_Checking_Assistant = client.beta.assistants.update(\n",
    "    assistant_id=Contract_Checking_Assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thread_BeqCLSVxeCKO70UKQBXUtEMM'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample thread with a non-empty message content\n",
    "thread = client.beta.threads.create()\n",
    "thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contract_Checking_Assistant = client.beta.assistants.update(\n",
    "    assistant_id=Contract_Checking_Assistant.id,\n",
    "    tools=[\n",
    "        {\"type\": \"file_search\"},\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"contract_check\",\n",
    "                \"description\": \"Check the contract against specified regulations.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"contract_text\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The text of the contract to be checked.\"\n",
    "                        },\n",
    "                        \"regulations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"description\": \"A list of regulations to check against.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"contract_text\", \"regulations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "du.configure_upload(app, 'uploads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_word(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from Word file: {e}\")\n",
    "        raise\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_large(text):\n",
    "    try:\n",
    "        model_name = \"xlm-roberta-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = TFAutoModel.from_pretrained(model_name)\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return embedding.numpy().tolist()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error embedding text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_check(contract_text, regulations):\n",
    "    embeddings = embed_text_large(contract_text)\n",
    "    logging.info(\"Embeddings generated for the contract text.\")\n",
    "    response = client.beta.assistants.runs.create(\n",
    "        assistant_id=assistant_id,\n",
    "        thread_id=thread.id,\n",
    "        function_call={\"name\": \"contract_check\", \"arguments\": {\"contract_text\": contract_text, \"embeddings\": embeddings}}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.layout = html.Div([\n",
    "    html.H1(\"Contract Checking Assistant\"),\n",
    "    dcc.Upload(\n",
    "        id='upload-file',\n",
    "        children=html.Div(['Drag and Drop or ', html.A('Select a Word File')]),\n",
    "        style={\n",
    "            'width': '100%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=False\n",
    "    ),\n",
    "    dcc.Input(id='user-message', type='text', placeholder='Enter your message with the contract'),\n",
    "    html.Button('Submit', id='submit-button', n_clicks=0),\n",
    "    html.Div(id='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output('output', 'children'),\n",
    "    Input('submit-button', 'n_clicks'),\n",
    "    State('upload-file', 'contents'),\n",
    "    State('user-message', 'value'))\n",
    "\n",
    "def update_output(n_clicks, contents, user_message):\n",
    "    if n_clicks > 0 and contents and user_message:\n",
    "        content_type, content_string = contents.split(',')\n",
    "        decoded = base64.b64decode(content_string)\n",
    "        file_path = 'uploaded_contract.docx'\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(decoded)\n",
    "        \n",
    "        try:\n",
    "            contract_text = extract_text_from_word(file_path)\n",
    "            return f\"Length of contract text: {len(contract_text)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error reading the file: {e}\"\n",
    "\n",
    "        embeddings = embed_text_large(contract_text)\n",
    "\n",
    "        if check_usage_limit() == 0:\n",
    "            return \"Usage limit reached. Please try again later.\"\n",
    "\n",
    "        try:\n",
    "            thread = client.beta.threads.create()\n",
    "        except Exception as e:\n",
    "            return f\"Error creating thread: {e}\"\n",
    "\n",
    "        try:\n",
    "            message = client.beta.threads.messages.create(\n",
    "                thread_id=thread.id,\n",
    "                role=\"user\",\n",
    "                content=f\"{user_message}\\n\\n{contract_text}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Error sending message to thread: {e}\"\n",
    "\n",
    "        run = client.beta.threads.runs.create(\n",
    "            assistant_id=assistant_id,\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "\n",
    "        while response.status != \"completed\":\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                response = client.beta.assistants.retrieve(\n",
    "                    assistant_id=assistant_id,\n",
    "                    run_id=run.id                   \n",
    "                )\n",
    "            except Exception as e:\n",
    "                return f\"Error retrieving response: {e}\"\n",
    "\n",
    "        return response.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\dash\\dash.py:2259\u001b[0m, in \u001b[0;36mDash.run_server\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_server\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"`run_server` is a deprecated alias of `run` and may be removed in a\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m \u001b[38;5;124;03m    future version. We recommend using `app.run` instead.\u001b[39;00m\n\u001b[0;32m   2256\u001b[0m \n\u001b[0;32m   2257\u001b[0m \u001b[38;5;124;03m    See `app.run` for usage information.\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\dash\\dash.py:2150\u001b[0m, in \u001b[0;36mDash.run\u001b[1;34m(self, host, port, proxy, debug, jupyter_mode, jupyter_width, jupyter_height, jupyter_server_url, dev_tools_ui, dev_tools_props_check, dev_tools_serve_dev_bundles, dev_tools_hot_reload, dev_tools_hot_reload_interval, dev_tools_hot_reload_watch_interval, dev_tools_hot_reload_max_retry, dev_tools_silence_routes_logging, dev_tools_prune_errors, **flask_run_options)\u001b[0m\n\u001b[0;32m   2147\u001b[0m             extra_files\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter_dash\u001b[38;5;241m.\u001b[39mactive:\n\u001b[1;32m-> 2150\u001b[0m     \u001b[43mjupyter_dash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_app\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjupyter_server_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun(host\u001b[38;5;241m=\u001b[39mhost, port\u001b[38;5;241m=\u001b[39mport, debug\u001b[38;5;241m=\u001b[39mdebug, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflask_run_options)\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\dash\\_jupyter.py:404\u001b[0m, in \u001b[0;36mJupyterDash.run_app\u001b[1;34m(self, app, mode, width, height, host, port, server_url)\u001b[0m\n\u001b[0;32m    402\u001b[0m     display(HTML(msg))\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_error\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\dash\\_jupyter.py:391\u001b[0m, in \u001b[0;36mJupyterDash.run_app\u001b[1;34m(self, app, mode, width, height, host, port, server_url)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[43mwait_for_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_colab:\n\u001b[0;32m    394\u001b[0m         JupyterDash\u001b[38;5;241m.\u001b[39m_display_in_colab(dashboard_url, port, mode, width, height)\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@six\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrying\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\retrying.py:266\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop(attempt_number, delay_since_first_attempt_ms):\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_exception \u001b[38;5;129;01mand\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39mhas_exception:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# get() on an attempt with an exception should cause it to be raised, but raise just in case\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RetryError(attempt)\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RetryError(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m         \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_attempts(attempt_number)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32mc:\\Users\\Aria\\.conda\\envs\\tensorenv\\Lib\\site-packages\\dash\\_jupyter.py:378\u001b[0m, in \u001b[0;36mJupyterDash.run_app.<locals>.wait_for_app\u001b[1;34m()\u001b[0m\n\u001b[0;32m    376\u001b[0m res \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(res)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    381\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
